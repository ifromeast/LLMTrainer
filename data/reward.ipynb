{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f37b6fe-27fe-49e9-970c-d57ab8c83e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/zzd-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, pretrain_or_model: str, from_config=False, normalize_reward=True, use_flash_attention_2=False, dtype=\"auto\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrain_or_model,\n",
    "            torch_dtype=dtype,\n",
    "            trust_remote_code=True,\n",
    "            use_flash_attention_2=use_flash_attention_2,\n",
    "        ).model\n",
    "        # value head\n",
    "        self.value_head = nn.Linear(self.model.config.hidden_size, 1)\n",
    "        self.value_head.weight.data.normal_(mean=0.0, std=1 / (self.model.config.hidden_size + 1))\n",
    "\n",
    "        # mean std\n",
    "        self.normalize_reward = normalize_reward\n",
    "        self.register_buffer(\"mean\", torch.zeros(1))\n",
    "        self.register_buffer(\"std\", torch.ones(1))\n",
    "\n",
    "    def forward(self, sequences: torch.LongTensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        outputs = self.model(sequences, attention_mask=attention_mask)\n",
    "        last_hidden_states = outputs[\"last_hidden_state\"]\n",
    "        values = self.value_head(last_hidden_states).squeeze(-1)\n",
    "\n",
    "        # left padding in training mode\n",
    "        if self.training:\n",
    "            reward = values[:, -1]\n",
    "        else:\n",
    "            # assume that there is some padding on both sides\n",
    "            last_value = []\n",
    "            for i in range(sequences.size(0)):\n",
    "                for t in reversed(range(sequences.size(1))):\n",
    "                    if attention_mask[i][t] > 0.5:\n",
    "                        last_value.append(values[i][t])\n",
    "                        break\n",
    "            reward = torch.stack(last_value)\n",
    "\n",
    "            # normalize reward in eval mode\n",
    "            if self.normalize_reward:\n",
    "                reward = (reward - self.mean) / self.std\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b9ffa1-5646-41f1-ab4a-31f42901ac5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  6.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight loading ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = 'cuda:1'\n",
    "\n",
    "base_model = '/data/share_user/zzd/ckpt/Baichuan2-13B-Chat'\n",
    "ckpt_path = '/data/share_user/zzd/ckpt/rlhf_baichuan2/1124/rm_model.pt'\n",
    "def init_model():\n",
    "    print(\"init model ...\")\n",
    "    model = RewardModel(base_model, dtype=torch.float16)\n",
    "    print(\"weight loading ...\")\n",
    "\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device), strict=True)\n",
    "    model.half().eval().to(device)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False, trust_remote_code=True)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = init_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9257d5df-90cc-499b-bd43-7182907e1058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset:\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "    def __init__(self, tokenizer, model_max_length=4096, user_tokens=[195], assistant_tokens=[196],):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_max_length = model_max_length\n",
    "        self.user_tokens = user_tokens\n",
    "        self.assistant_tokens = assistant_tokens\n",
    "        self.ignore_index = -100\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def preprocessing(self, messages):\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "\n",
    "        for message in messages:\n",
    "            from_ = message[\"role\"]\n",
    "            value = message[\"content\"]\n",
    "            value_ids = self.tokenizer.encode(value)\n",
    "\n",
    "            if from_ == \"user\":\n",
    "                input_ids += self.user_tokens + value_ids\n",
    "                labels += [self.tokenizer.eos_token_id] + [self.ignore_index] * len(value_ids)\n",
    "            else:\n",
    "                input_ids += self.assistant_tokens + value_ids\n",
    "                labels += [self.ignore_index] + value_ids\n",
    "        input_ids.append(self.tokenizer.eos_token_id)\n",
    "        labels.append(self.tokenizer.eos_token_id)\n",
    "        input_ids = input_ids[: self.model_max_length]\n",
    "        labels = labels[: self.model_max_length]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.model_max_length - len(input_ids))\n",
    "        labels += [self.ignore_index] * (self.model_max_length - len(labels))\n",
    "        input_ids = torch.LongTensor(input_ids)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)\n",
    "        return input_ids, labels, attention_mask\n",
    "\n",
    "dataset = MyDataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f1fd0f-ae8d-4374-bc0d-9784be6de4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]/tmp/ipykernel_529225/2859308612.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['score'][i] = score_map\n",
      "/tmp/ipykernel_529225/2859308612.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '{'sft[1]': 1.253, 'gpt4[2]': 1.23, 'Yi-34B[3]': 1.264, 'Aquila-34B[4]': 1.27, 'deepseek-67B[5]': 1.318}' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df['score'][i] = score_map\n",
      "100%|██████████| 150/150 [12:45<00:00,  5.11s/it]\n"
     ]
    }
   ],
   "source": [
    "file_name = 'youxiake'\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_excel(f'{file_name}_tmp.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "model_list = ['sft[1]', 'gpt4[2]', 'Yi-34B[3]', 'Aquila-34B[4]', 'deepseek-67B[5]']\n",
    "\n",
    "df['score'] = 0\n",
    "for i in tqdm(range(len(df))):\n",
    "    score_map = {'sft[1]':0.0, 'gpt4[2]':0.0, 'Yi-34B[3]':0.0, 'Aquila-34B[4]':0.0, 'deepseek-67B[5]':0.0}\n",
    "    for model_name in model_list:\n",
    "        messages = [{\"role\": \"user\", \"content\": df['query'][i]}, {'role': 'assistant','content':df[model_name][i]}]\n",
    "        input_ids, labels, attention_mask = dataset.preprocessing(messages)\n",
    "        # print(input_ids, attention_mask)\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            reward = model(input_ids.unsqueeze(0).to(device), attention_mask.unsqueeze(0).to(device))\n",
    "        score_map[model_name] = reward.detach().cpu().numpy()[0]\n",
    "    df['score'][i] = score_map\n",
    "\n",
    "save_path = f'{file_name}_score.xlsx'\n",
    "writer = pd.ExcelWriter(save_path)\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10173324-4782-44ce-ae82-ebca978d56f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardModel(\n",
       "  (model): BaichuanModel(\n",
       "    (embed_tokens): Embedding(125696, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x BaichuanLayer(\n",
       "        (self_attn): BaichuanAttention(\n",
       "          (W_pack): Linear(in_features=5120, out_features=15360, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13696, bias=False)\n",
       "          (down_proj): Linear(in_features=13696, out_features=5120, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13696, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (value_head): Linear(in_features=5120, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b9b196-b058-47b0-a6f3-42b8452fca34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zzd-env",
   "language": "python",
   "name": "zzd-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
