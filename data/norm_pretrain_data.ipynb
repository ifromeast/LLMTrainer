{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import argparse\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from distinct import Distinct\n",
    "\n",
    "\n",
    "\n",
    "dataset_path = Path( '/data/pretrain_data/')\n",
    "new_dataset_path = Path(\"/data/pretrain_data/norm_data/\")\n",
    "\n",
    "# data_files = [file.name for file in dataset_path.glob(\"*.jsonl\")]\n",
    "data_files = ['dedup_cleaned_route_final.jsonl']\n",
    "total_length = len(data_files)\n",
    "for file in data_files:\n",
    "    print('norm:', file)\n",
    "    raw_len = 0\n",
    "    idx = 0\n",
    "    tmp_list = []\n",
    "    \n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as fcc_file:\n",
    "    #     data = json.load(fcc_file)\n",
    "\n",
    "    #     for d in data:\n",
    "        for data in jsonlines.Reader(fcc_file):\n",
    "            d=data\n",
    "            raw_len += 1\n",
    "            if data['zh_ratio'] > 0.6 and d['distinct'] > 0.6 and data['length'] > 200 :\n",
    "                norm_d = {'dataType':data['dataType'],\n",
    "                          'content': unicodedata.normalize('NFKC', d['content']),}\n",
    "                tmp_list.append(norm_d)\n",
    "#             if len(tmp_list) >= 700000:\n",
    "#                 print('Raw:', raw_len)\n",
    "#                 print('New:', len(tmp_list), 'ratio:', len(tmp_list)/raw_len)\n",
    "#                 new_name = file.split('.')[0] + '_norm'+ f'_{idx}'\n",
    "#                 new_name += '.json'\n",
    "#                 new_file_path = os.path.join(new_dataset_path, new_name)\n",
    "#                 with open(new_file_path, \"w\", encoding=\"utf-8\") as dump_f:\n",
    "#                     json.dump(tmp_list, dump_f, ensure_ascii=False)\n",
    "#                 raw_len = 0\n",
    "#                 tmp_list = []\n",
    "#                 idx += 1\n",
    "\n",
    "    print('Raw:', raw_len)\n",
    "    print('New:', len(tmp_list), 'ratio:', len(tmp_list)/raw_len)\n",
    "    new_name = file.split('.')[0] + '_norm' #+ f'_{idx}'\n",
    "    new_name += '.json'\n",
    "    new_file_path = os.path.join(new_dataset_path, new_name)\n",
    "    with open(new_file_path, \"w\", encoding=\"utf-8\") as dump_f:\n",
    "        json.dump(tmp_list, dump_f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "norm: dedup_cleaned_route_final.jsonl\n",
    "Raw: 276129\n",
    "New: 203445 ratio: 0.7367752028942994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "data_file = '/data/share_user/zzd/data/pretrain_data/norm_data/' + new_name\n",
    "raw_dataset = load_dataset(\"json\", data_files=data_file, keep_in_memory=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
