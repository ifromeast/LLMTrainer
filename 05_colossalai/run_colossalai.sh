colossalai run --nproc_per_node 8 pretrain_colossalai.py \
    --config /root/alpaca_test/LLMTrainer/config/config.json \
    --plugin gemini_cuda \
    --batch_size 28 \
    --lr 2e-5 \
    --weigth_decay 0.01 \
    --warmup_steps 2 \
    --grad_checkpoint \
    --max_length 2048 \
    --mixed_precision fp16 \
    --flash_attention